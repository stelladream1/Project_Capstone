
import sys    # 시스템
import os     # 시스템

import pandas as pd    # 판다스 : 데이터분석 라이브러리
import numpy as np     # 넘파이 : 숫자, 행렬 데이터 라이브러리
import datetime
from selenium.webdriver.common.keys import Keys
import urllib.request
from bs4 import BeautifulSoup     # html 데이터 전처리
from selenium import webdriver    # 웹 브라우저 자동화
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time                       # 시간 지연
from tqdm import tqdm_notebook    # 진행상황 표시
from tqdm.notebook import tqdm
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from selenium.webdriver.support.ui import WebDriverWait 
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from konlpy.tag import Okt 
import requests
import json

# python 버전 확인
# !python --version

# 판다스 버전 확인
# pd.__version__


# 크롬 웹브라우저 실행
#driver = webdriver.Chrome(executable_path='c:/temp/chromedriver_85/chromedriver.exe')
driver = webdriver.Chrome(ChromeDriverManager().install())
# 사이트 주소
driver.get("https://class101.net/search?category=604f1c9756c3676f1ed003b3")
time.sleep(2)

def scroll_down(driver):
    driver.execute_script("window.scrollTo(0, 7200)")
    time.sleep(1)


def doScrollDown(whileSeconds):
        start = datetime.datetime.now()
        end = start + datetime.timedelta(seconds=whileSeconds)
        while True:
            driver.execute_script("window.scrollTo(0, 800)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(800, 1600)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(1600, 2400)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(2400, 3200)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(3200, 4000)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(4000, 4800)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(4800, 5600)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(5600, 6400)")
            time.sleep(1)
            driver.execute_script("window.scrollTo(6400, 6800)")
            time.sleep(1)
            if datetime.datetime.now() > end:
                break

def find_longest_word(words):
	word_len = []
	for n in words:
		word_len.append((len(n), n))
	word_len.sort()
	return word_len[-1][1]

platform_class = "클래스 101"
category = "3"
category_id = []
platform_name = []
url = []
name = []
price = []
img = []
instructor_name = []
noun_list = []
content = []
curPage = 1
totalPage = 1
number = 1 
watch_time = []
end_time = []
id_list = []

token = None
while curPage <= totalPage :
    
    time.sleep(10)
    doScrollDown(1)

    article_title = driver.find_elements_by_css_selector("div.sc-6af7b87c-4.lbREYw")
    article_title[0]
    
    for article in article_title:
        title = article.text
        name.append(title)
           
        detail = platform_class  + " " +title
        content.append(detail)
          
        watch_time1 = 0
        watch_time.append(watch_time1)
        
        end_time1 = "none" 
        end_time.append(end_time1)
         
        
        category_id.append(category)
        
        platform_name.append(platform_class)

        
        okt = Okt()
        
        
        noun = okt.nouns(title)
        find_noun =  find_longest_word(noun)
        
        noun_list.append(find_noun)
        
        
        
        
        
    time.sleep(1)

    article_name = driver.find_elements_by_css_selector("p.css-g5ijt3")
    article_name[0]

    for article in article_name:
        name_ins = article.text
        instructor_name.append(name_ins)

    time.sleep(1)


    article_url = driver.find_elements_by_css_selector("a.sc-58f0a406-0.dOHkOE")
    article_url[0]

    for article in article_url:
        url_class = article.get_attribute('href')   
        url.append(url_class)


    time.sleep(1)

    article_main_price = driver.find_elements_by_css_selector("li.sc-fba4ac18-2.kaBggN")
    article_main_price[0]

    for i in article_main_price:
        try :
            price_class= i.find_element_by_class_name("css-58su99").text
            price.append(price_class)
        except:
            price.append("0원")
    time.sleep(1)

    article_img =  driver.find_elements_by_xpath("//span[@class='sc-29823a73-0 bUogJm sc-630e1992-2 kKdHVx']/picture[@class='sc-1ceaa036-0 kWswCZ sc-29823a73-1 iXXLck']/img") 

    article_img[0]
    address = "C:\py_temp\img"

    for article in article_img :
            try : 
                                                
                img_class = article.get_attribute('src')   
                img.append(img_class)                               
            except: 
                pass
            continue

   
    number = number + 1
    curPage += 1
     
        
    


    if curPage > totalPage:
        print('Crawling succeed!')
        break
   
    cur_css = '//*[@id="wrapper"]/div/div[2]/div/section/section/section[2]/div/div[3]/div[3]/section/div/button[{}]'.format(number)

    driver.find_element_by_xpath(cur_css).click()
    time.sleep(2)
        
driver.close()   

print("url개수: ", len(url))

df3 = pd.DataFrame({'img' : img})

df3.to_excel("class3.xlsx")  
time.sleep(2)

load = pd.read_excel("class3.xlsx")

num_list = len(load)

number = num_list    # 수집할 글 갯수




address = "C:\py_temp"
# 수집한 url 돌면서 데이터 수집

for i in tqdm_notebook(range(0, number)):
    # 글 띄우기
        img = load ["img"][i]
        urllib.request.urlretrieve(img,address+"/"+str(i)+'.jpg') 


import os
import glob
import json
from PIL import Image

files = glob.glob('C:\py_temp/*.jpg')

s = requests.Session()
s.mount('http://demo-257674007.ap-northeast-2.elb.amazonaws.com', HTTPAdapter(max_retries=5))

for i in files:
    img = Image.open(i)
    img_resize = img.resize((int(img.width / 2), int(img.height / 2)))
    title, ext = os.path.splitext(i)
    im = img_resize.convert("RGB")
    im.save(title + '_half' + ext)

loginUrl = 'http://demo-257674007.ap-northeast-2.elb.amazonaws.com/api/login'
payload = {"email":"class101@gmail.com" ,"password":"123123123"}

r = requests.post(
    loginUrl, data=json.dumps(payload),
    headers={'Content-Type': 'application/json'}
    )
token = json.loads(json.dumps(r.json()))['token']

print(token)

imageUrl = 'http://demo-257674007.ap-northeast-2.elb.amazonaws.com/api/uploadThumbNail'
    
createUrl = 'http://demo-257674007.ap-northeast-2.elb.amazonaws.com/api/uploadLecture'
    
subUrl = 'http://demo-257674007.ap-northeast-2.elb.amazonaws.com/api/uploadSubCategory'   
    
for i in range(len(load)):
    
        file_name = str(i) + '_half.jpg'

        with open(file_name, 'rb') as img:

            name_img= os.path.basename(file_name)

            files= {'upload': (name_img,img,'multipart/form-data')}

            headers = {'Accept': '*/*','Authorization': 'Bearer ' + token}

            with requests.Session() as s:
                r = s.post(imageUrl,files=files,headers=headers,verify=False)
                print(r.json())
                id = json.loads(json.dumps(r.json()))['id']
                print(id)
                idlist = id
                id_list.append(idlist)
            
        i = i + 1   
    
dict_list = []

for i in range(len(load)):
    dict_list.append({'name':name[i], 'instructor_name':instructor_name[i], 'content':content[i], 'category_id':9,'file_id':id_list[i],'platform_name': [platform_name[i]],'price': [price[i]],'url': [url[i]], 'watch_time': [0]})

for i in files:
        with open("dict_list.json", "w", encoding='UTF-8-sig') as f:

            json.dump(dict_list,f,indent=4,ensure_ascii=False)
 
        with open('dict_list.json', 'rt',encoding= "UTF-8-sig") as s:
            headers = {'Accept': '*/*','Content-Type': 'application/json','Authorization': 'Bearer ' + token}
            print(token)
            datas1 = json.load(s)
            
            for n in datas1:
                datas = n
                print(datas)
                
    
                with requests.Session() as s: 
                    r = s.post(createUrl,json = datas, headers=headers,verify=False)
                    print(r.json())
                    id = json.loads(json.dumps(r.json()))['id']
                    print(id)
                    time.sleep(3)
                    
time.sleep(10)            
            
noun_list3 = []                    
                    
                    
for i in range(len(load)):
    noun_list3.append({'name':noun_list[i],'category_id':9})   
    

with open("noun_list3.json", "w", encoding='UTF-8-sig') as f:

    json.dump(noun_list3,f,indent=4,ensure_ascii=False)

with open('noun_list3.json', 'rt',encoding= "UTF-8-sig") as s:
        
            headers = {'Accept': '*/*','Content-Type': 'application/json','Authorization': 'Bearer ' + token}
            
            print(token)
            datas1 = json.load(s)
            
            for n in datas1:
                datas = n
                print(datas)
                
                with requests.Session() as s: 
                    r = s.post(subUrl,json = datas, headers=headers)
                    print('finish')
                    time.sleep(3)
                    
noun_list.clear()                    
noun_list3.clear()
             
                    
